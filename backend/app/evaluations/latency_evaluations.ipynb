{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 queries loaded\n"
     ]
    }
   ],
   "source": [
    "script_dir = os.path.dirname('.')\n",
    "df = pd.read_csv(os.path.join(script_dir, 'datasets', 'Meezan-HR-synthetic-testset.csv'))\n",
    "queries = df['question'].tolist()\n",
    "print(f\"{len(queries)} queries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the dress code for female employees in the workplace?', 'What is the purpose and process of a domestic inquiry?', 'What is the role of a management representative in the inquiry proceedings?', 'What are the vehicle arrangement options for employees in EVP grade and above?', 'What are the incentives provided for passing the Junior Associateship of IBP exams?', \"What are the consequences for employees who don't follow office discipline, especially regarding punctuality and reading newspapers during office hours?\", \"What support does Meezan Bank provide to an employee's family in case of death, and where does it come from?\", 'What is the purpose of the HR Guidelines & Procedure Document at Meezan Bank Limited?', 'What are the guidelines for recruitment, selection, and placement in the organization, including dress code and office timings?', \"What are the steps and requirements for an employee to receive a cash incentive in the company's sales commission structure?\"]\n"
     ]
    }
   ],
   "source": [
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_completion(chat_id: int, query: str, eval_approach: str):\n",
    "    '''\n",
    "    makes post request to RAG app endpoint to get AI response. \n",
    "    Passes chat_id, query and use_litellm flag in the request body.\n",
    "    '''\n",
    "    url = f'http://localhost:8000/api/chat/{chat_id}/get_ai_response/'\n",
    "    data = {\n",
    "        'query': query,\n",
    "        'eval_approach': eval_approach\n",
    "    }\n",
    "    response = requests.post(url=url, data=data)\n",
    "    return response\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAT_ID = 4  # existing Meezan HR chat id\n",
    "\n",
    "response = rag_completion(chat_id=CHAT_ID, query=queries[0], eval_approach='litellm_proxy')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The dress code for female employees in the workplace includes the following guidelines:\\n\\n1. **Elegant Dressing**: Female employees should dress elegantly with a Headscarf and Abaya, without being ostentatious.\\n\\n2. **Modest Attire**: An appropriate dress code includes Shalwar Kameez or any modest dressing, along with an Abaya.\\n\\n3. **Headscarf and Abaya**: Female staff are required to wear Hijabs (a scarf covering the entire head and hair) and a gown covering sleeves while on duty, training, and on clients' visits, as required by Islamic injunctions.\\n\\n4. **Discretion with Makeup and Jewelry**: Female staff are expected to exercise discretion in their choice of makeup and jewelry. Nails should not be longer than medium and may only be polished with neutral colors.\\n\\n5. **Body Hygiene**: Proper care of body hygiene is a must for all female employees.\\n\\nThese guidelines aim to maintain a professional appearance while also respecting cultural and religious considerations in the workplace.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json().get('ai_message').get('content')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate latency for open AI RAG approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken for 10 queries with direct OpenAI RAG approach: 45.16123628616333 seconds\n"
     ]
    }
   ],
   "source": [
    "CHAT_ID = 4  # existing Meezan HR chat id\n",
    "\n",
    "responses = []\n",
    "start_time = time.time()\n",
    "for query in queries:\n",
    "    response = rag_completion(chat_id=CHAT_ID, query=query, eval_approach='openai')\n",
    "    responses.append(response.json().get('ai_message').get('content'))\n",
    "\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "df['openai_rag_response'] = responses\n",
    "df.to_csv(os.path.join(script_dir, 'datasets', 'Meezan-HR-synthetic-testset.csv'), index=False)\n",
    "\n",
    "print(f\"Total time taken for {len(queries)} queries with direct OpenAI RAG approach: {time_taken} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate latency for litellm proxy RAG approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken for 10 queries with litellm proxy approach: 56.30036687850952 seconds\n"
     ]
    }
   ],
   "source": [
    "CHAT_ID = 4  # existing Meezan HR chat id\n",
    "\n",
    "responses = []\n",
    "start_time = time.time()\n",
    "for query in queries:\n",
    "    response = rag_completion(chat_id=CHAT_ID, query=query, eval_approach='litellm_proxy')\n",
    "    responses.append(response.json().get('ai_message').get('content'))\n",
    "\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "df['litellm_proxy_rag_response'] = responses\n",
    "df.to_csv(os.path.join(script_dir, 'datasets', 'Meezan-HR-synthetic-testset.csv'), index=False)\n",
    "\n",
    "print(f\"Total time taken for {len(queries)} queries with litellm proxy approach: {time_taken} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate latency for litellm proxy RAG approach with fallback being used (set up wrong api in proxy for gpt-4-omni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken for 10 queries with litellm proxy with fallback approach: 172.13977551460266 seconds\n"
     ]
    }
   ],
   "source": [
    "CHAT_ID = 4  # existing Meezan HR chat id\n",
    "\n",
    "responses = []\n",
    "start_time = time.time()\n",
    "for query in queries:\n",
    "    response = rag_completion(chat_id=CHAT_ID, query=query, eval_approach='litellm_proxy')\n",
    "    responses.append(response.json().get('ai_message').get('content'))\n",
    "\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "df['openrouter_mistral_rag_response'] = responses\n",
    "df.to_csv(os.path.join(script_dir, 'datasets', 'Meezan-HR-synthetic-testset.csv'), index=False)\n",
    "\n",
    "print(f\"Total time taken for {len(queries)} queries with litellm proxy with fallback approach: {time_taken} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate latency for litellm proxy RAG approach with non gpt model (openrouter mistral 7b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Techwards/Multi-LLM-Routing-RAG/venv/lib/python3.10/site-packages/requests/models.py:974\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# try one call first\u001b[39;00m\n\u001b[1;32m      2\u001b[0m response \u001b[38;5;241m=\u001b[39m rag_completion(chat_id\u001b[38;5;241m=\u001b[39mCHAT_ID, query\u001b[38;5;241m=\u001b[39mqueries[\u001b[38;5;241m0\u001b[39m], eval_approach\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlitellm_proxy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Techwards/Multi-LLM-Routing-RAG/venv/lib/python3.10/site-packages/requests/models.py:978\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[0;32m--> 978\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "# try one call first\n",
    "response = rag_completion(chat_id=4, query=queries[0], eval_approach='litellm_proxy')\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken for 10 queries with litellm proxy with nongpt approach: 71.99534797668457 seconds\n"
     ]
    }
   ],
   "source": [
    "# litellm_proxy_nongpt approach\n",
    "CHAT_ID = 4  # existing Meezan HR chat id\n",
    "\n",
    "responses = []\n",
    "start_time = time.time()\n",
    "for query in queries:\n",
    "    response = rag_completion(chat_id=CHAT_ID, query=query, eval_approach='litellm_proxy_nongpt')\n",
    "    responses.append(response.json().get('ai_message').get('content'))\n",
    "    \n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "print(f\"Total time taken for {len(queries)} queries with litellm proxy with nongpt approach: {time_taken} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate latency for litellm proxy RAG approach with bedrock llama 70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have information about my creators or the underlying implementation details. I'm a chatbot designed to provide helpful and informative responses to user queries, and I don't have personal knowledge or information about my own creation.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try one call first\n",
    "response = rag_completion(chat_id=4, query='who made you?', eval_approach='litellm_proxy_bedrock_llama70b')\n",
    "response.json().get('ai_message').get('content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken for 5 queries with litellm proxy with bedrock_llama70b approach: 20.836982011795044 seconds\n"
     ]
    }
   ],
   "source": [
    "CHAT_ID = 4  # existing Meezan HR chat id\n",
    "\n",
    "responses = []\n",
    "start_time = time.time()\n",
    "for query in queries[:len(queries)//2]:\n",
    "    response = rag_completion(chat_id=CHAT_ID, query=query, eval_approach='litellm_proxy_bedrock_llama70b')\n",
    "    responses.append(response.json().get('ai_message').get('content'))\n",
    "    \n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "print(f\"Total time taken for {len(queries) // 2} queries with litellm proxy with bedrock_llama70b approach: {time_taken} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken for 5 queries with litellm proxy with bedrock_llama70b approach: 25.90020728111267 seconds\n"
     ]
    }
   ],
   "source": [
    "CHAT_ID = 4  # existing Meezan HR chat id\n",
    "\n",
    "responses = []\n",
    "start_time = time.time()\n",
    "for query in queries[len(queries)//2:]:\n",
    "    response = rag_completion(chat_id=CHAT_ID, query=query, eval_approach='litellm_proxy_bedrock_llama70b')\n",
    "    responses.append(response.json().get('ai_message').get('content'))\n",
    "    \n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "print(f\"Total time taken for {len(queries) // 2} queries with litellm proxy with bedrock_llama70b approach: {time_taken} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(queries[::2] + queries[1::2]) == set(queries)\n",
    "queries[:len(queries)//2] + queries[len(queries)//2:] == queries\n",
    "len(query[:len(queries)//2]), len(queries[len(queries)//2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate latency for litellm proxy RAG approach with fallback to bedrock llama 70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken for 10 queries with litellm proxy with fallback to bedrock_llama70b approach: 170.07091736793518 seconds\n"
     ]
    }
   ],
   "source": [
    "CHAT_ID = 4  # existing Meezan HR chat id\n",
    "\n",
    "responses = []\n",
    "start_time = time.time()\n",
    "for query in queries:\n",
    "    response = rag_completion(chat_id=CHAT_ID, query=query, eval_approach='litellm_proxy_gpt-4') \n",
    "    responses.append(response.json().get('ai_message').get('content'))\n",
    "    \n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "df['bedrock_llama70b_rag_response'] = responses\n",
    "df.to_csv(os.path.join(script_dir, 'datasets', 'Meezan-HR-synthetic-testset.csv'), index=False)\n",
    "\n",
    "print(f\"Total time taken for {len(queries)} queries with litellm proxy with fallback to bedrock_llama70b approach: {time_taken} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
